#!/usr/bin/env python3
"""Monitor and check status of stage executions."""

import argparse
import json
from pathlib import Path
from datetime import datetime


def check_stage_outputs(output_dir: Path):
    """Check what stage outputs exist in directory."""
    
    stage_files = {
        'markdown-conversion': list(output_dir.glob("*.md")),
        'markdown-optimizer': list(output_dir.glob("*.opt.md")),
        'chunker': list(output_dir.glob("*.chunks.json")),
        'fact-generator': list(output_dir.glob("*.facts.json")),
        'ingestor': list(output_dir.glob("*.ingestion.json"))
    }
    
    print(f"üìÅ Checking stage outputs in: {output_dir}")
    print("=" * 60)
    
    stage_descriptions = {
        'markdown-conversion': "Markdown Conversion",
        'markdown-optimizer': "Markdown Optimizer", 
        'chunker': "Chunking",
        'fact-generator': "Fact Generation",
        'ingestor': "Ingestion"
    }
    
    for stage, files in stage_files.items():
        description = stage_descriptions[stage]
        
        if files:
            print(f"‚úÖ {stage} ({description}): {len(files)} files")
            for file in files[:3]:  # Show first 3 files
                print(f"   üìÑ {file.name}")
            if len(files) > 3:
                print(f"   ... and {len(files) - 3} more")
        else:
            print(f"‚ùå {stage} ({description}): No outputs found")
    
    print("=" * 60)


def analyze_stage_file(file_path: Path):
    """Analyze a specific stage output file."""
    
    print(f"üìÑ Analyzing: {file_path.name}")
    print("-" * 40)
    
    if file_path.suffix == '.md':
        # Analyze markdown file
        content = file_path.read_text(encoding='utf-8')
        
        # Extract metadata
        if content.startswith('---'):
            parts = content.split('---', 2)
            if len(parts) >= 3:
                import yaml
                try:
                    metadata = yaml.safe_load(parts[1])
                    content_text = parts[2].strip()
                    
                    print(f"üìä Metadata:")
                    for key, value in metadata.items():
                        print(f"   {key}: {value}")
                    
                    print(f"\nüìù Content Stats:")
                    print(f"   Word count: {len(content_text.split())}")
                    print(f"   Character count: {len(content_text)}")
                    print(f"   Line count: {len(content_text.splitlines())}")
                    
                    # Check for timestamps
                    timestamp_count = content_text.count('[') + content_text.count('##')
                    if timestamp_count > 0:
                        print(f"   Timestamps/Headers: ~{timestamp_count}")
                        
                except yaml.YAMLError as e:
                    print(f"   ‚ö†Ô∏è  Could not parse metadata: {e}")
        else:
            print(f"üìù Content Stats:")
            print(f"   Word count: {len(content.split())}")
            print(f"   Character count: {len(content)}")
            print(f"   Line count: {len(content.splitlines())}")
    
    elif file_path.suffix == '.json':
        # Analyze JSON file
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"üìä JSON Structure:")
            for key, value in data.items():
                if isinstance(value, list):
                    print(f"   {key}: {len(value)} items")
                    if len(value) > 0 and isinstance(value[0], dict):
                        # Show sample keys from first item
                        sample_keys = list(value[0].keys())[:5]
                        print(f"      Sample keys: {sample_keys}")
                elif isinstance(value, dict):
                    print(f"   {key}: {len(value)} keys")
                    sample_keys = list(value.keys())[:5]
                    print(f"      Keys: {sample_keys}")
                else:
                    print(f"   {key}: {type(value).__name__}")
                    
            # Special analysis for different file types
            if file_path.name.endswith('.chunks.json'):
                analyze_chunks_file(data)
            elif file_path.name.endswith('.facts.json'):
                analyze_facts_file(data)
            elif file_path.name.endswith('.ingestion.json'):
                analyze_ingestion_file(data)
                
        except json.JSONDecodeError as e:
            print(f"   ‚ùå Could not parse JSON: {e}")
        except Exception as e:
            print(f"   ‚ùå Error analyzing file: {e}")


def analyze_chunks_file(data):
    """Analyze chunks JSON file."""
    print(f"\nüß© Chunks Analysis:")
    
    if 'chunks' in data:
        chunks = data['chunks']
        print(f"   Total chunks: {len(chunks)}")
        
        if chunks:
            # Analyze chunk sizes
            chunk_sizes = [len(chunk.get('content', '')) for chunk in chunks]
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            print(f"   Average chunk size: {avg_size:.0f} characters")
            print(f"   Size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters")
            
            # Check for embeddings
            has_embeddings = any('embedding' in chunk for chunk in chunks)
            print(f"   Has embeddings: {'Yes' if has_embeddings else 'No'}")
    
    if 'summary' in data:
        summary = data['summary']
        print(f"   Summary length: {len(summary)} characters")


def analyze_facts_file(data):
    """Analyze facts JSON file."""
    print(f"\nüîç Facts Analysis:")
    
    if 'facts' in data:
        facts = data['facts']
        print(f"   Total facts: {len(facts)}")
        
        if facts:
            # Analyze fact types
            fact_types = {}
            for fact in facts:
                fact_type = fact.get('type', 'unknown')
                fact_types[fact_type] = fact_types.get(fact_type, 0) + 1
            
            print(f"   Fact types:")
            for fact_type, count in sorted(fact_types.items()):
                print(f"      {fact_type}: {count}")
    
    if 'entities' in data:
        entities = data['entities']
        print(f"   Total entities: {len(entities)}")
        
        if entities:
            # Analyze entity types
            entity_types = {}
            for entity in entities:
                entity_type = entity.get('type', 'unknown')
                entity_types[entity_type] = entity_types.get(entity_type, 0) + 1
            
            print(f"   Entity types:")
            for entity_type, count in sorted(entity_types.items()):
                print(f"      {entity_type}: {count}")
    
    if 'relations' in data:
        relations = data['relations']
        print(f"   Total relations: {len(relations)}")


def analyze_ingestion_file(data):
    """Analyze ingestion JSON file."""
    print(f"\nüíæ Ingestion Analysis:")
    
    if 'success' in data:
        print(f"   Success: {data['success']}")
    
    if 'chunks_ingested' in data:
        print(f"   Chunks ingested: {data['chunks_ingested']}")
    
    if 'entities_ingested' in data:
        print(f"   Entities ingested: {data['entities_ingested']}")
    
    if 'relations_ingested' in data:
        print(f"   Relations ingested: {data['relations_ingested']}")
    
    if 'processing_time' in data:
        print(f"   Processing time: {data['processing_time']:.2f}s")
    
    if 'databases' in data:
        databases = data['databases']
        print(f"   Target databases: {', '.join(databases)}")


def compare_stage_outputs(output_dir: Path):
    """Compare outputs across different stages."""
    
    print(f"üîÑ Comparing stage outputs in: {output_dir}")
    print("=" * 60)
    
    # Find all stage output files
    md_files = list(output_dir.glob("*.md"))
    opt_files = list(output_dir.glob("*.opt.md"))
    chunk_files = list(output_dir.glob("*.chunks.json"))
    fact_files = list(output_dir.glob("*.facts.json"))
    ingestion_files = list(output_dir.glob("*.ingestion.json"))
    
    # Group by base filename
    base_names = set()
    for file_list in [md_files, opt_files, chunk_files, fact_files, ingestion_files]:
        for file in file_list:
            base_name = file.name.split('.')[0]
            base_names.add(base_name)
    
    for base_name in sorted(base_names):
        print(f"\nüìÑ {base_name}:")
        
        # Check which stages have outputs
        stages_completed = []
        if any(f.name.startswith(base_name) and f.name.endswith('.md') for f in md_files):
            stages_completed.append("markdown-conversion")
        if any(f.name.startswith(base_name) and f.name.endswith('.opt.md') for f in opt_files):
            stages_completed.append("markdown-optimizer")
        if any(f.name.startswith(base_name) and f.name.endswith('.chunks.json') for f in chunk_files):
            stages_completed.append("chunker")
        if any(f.name.startswith(base_name) and f.name.endswith('.facts.json') for f in fact_files):
            stages_completed.append("fact-generator")
        if any(f.name.startswith(base_name) and f.name.endswith('.ingestion.json') for f in ingestion_files):
            stages_completed.append("ingestor")
        
        print(f"   Completed stages: {', '.join(stages_completed)}")
        print(f"   Progress: {len(stages_completed)}/5 stages")


def main():
    parser = argparse.ArgumentParser(description="Check stage execution status")
    parser.add_argument("--output-dir", default="./output", help="Output directory to check")
    parser.add_argument("--file", help="Specific file to analyze")
    parser.add_argument("--compare", action="store_true", help="Compare outputs across stages")
    
    args = parser.parse_args()
    
    if args.file:
        analyze_stage_file(Path(args.file))
    elif args.compare:
        compare_stage_outputs(Path(args.output_dir))
    else:
        check_stage_outputs(Path(args.output_dir))


if __name__ == "__main__":
    main()
