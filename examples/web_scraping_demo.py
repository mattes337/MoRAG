#!/usr/bin/env python3
"""
Web Scraping Demo for MoRAG

This script demonstrates the web scraping capabilities of MoRAG.
It shows how to extract content from web pages, convert to markdown,
and process the content for RAG applications.
"""

import asyncio
import json
from pathlib import Path
import sys

# Add the src directory to the path so we can import MoRAG modules
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from morag_web import WebProcessor, WebScrapingConfig


async def demo_basic_scraping():
    """Demonstrate basic web scraping functionality."""
    print("üåê MoRAG Web Scraping Demo")
    print("=" * 50)
    
    # Create web processor
    web_processor = WebProcessor()
    
    # Configure scraping with conservative settings
    config = WebScrapingConfig(
        timeout=10,
        max_retries=2,
        rate_limit_delay=1.0,
        extract_links=True,
        convert_to_markdown=True,
        clean_content=True,
        remove_navigation=True,
        remove_footer=True
    )
    
    # Example URLs to scrape (using public, scraping-friendly sites)
    test_urls = [
        "https://httpbin.org/html",  # Simple test HTML
        "https://example.com",       # Basic example site
    ]
    
    print(f"üìã Configuration:")
    print(f"   ‚Ä¢ Timeout: {config.timeout}s")
    print(f"   ‚Ä¢ Max retries: {config.max_retries}")
    print(f"   ‚Ä¢ Rate limit: {config.rate_limit_delay}s")
    print(f"   ‚Ä¢ Extract links: {config.extract_links}")
    print(f"   ‚Ä¢ Convert to markdown: {config.convert_to_markdown}")
    print()
    
    for i, url in enumerate(test_urls, 1):
        print(f"üîç Processing URL {i}/{len(test_urls)}: {url}")
        print("-" * 50)
        
        try:
            # Process the URL
            result = await web_processor.process_url(url, config)
            
            if result.success:
                content = result.content
                
                print(f"‚úÖ Success!")
                print(f"   ‚Ä¢ Title: {content.title}")
                print(f"   ‚Ä¢ Content length: {content.content_length} characters")
                print(f"   ‚Ä¢ Content type: {content.content_type}")
                print(f"   ‚Ä¢ Links found: {len(content.links)}")
                print(f"   ‚Ä¢ Images found: {len(content.images)}")
                print(f"   ‚Ä¢ Chunks created: {len(result.chunks)}")
                print(f"   ‚Ä¢ Processing time: {result.processing_time:.2f}s")
                print()
                
                # Show metadata
                print("üìä Metadata:")
                for key, value in content.metadata.items():
                    if key not in ['extracted_at']:  # Skip timestamp
                        print(f"   ‚Ä¢ {key}: {value}")
                print()
                
                # Show first 200 characters of content
                print("üìÑ Content preview:")
                preview = content.content[:200]
                if len(content.content) > 200:
                    preview += "..."
                print(f"   {preview}")
                print()
                
                # Show markdown preview if available
                if content.markdown_content:
                    print("üìù Markdown preview:")
                    md_preview = content.markdown_content[:200]
                    if len(content.markdown_content) > 200:
                        md_preview += "..."
                    print(f"   {md_preview}")
                    print()
                
                # Show chunk information
                if result.chunks:
                    print("üß© Chunks:")
                    for j, chunk in enumerate(result.chunks[:3]):  # Show first 3 chunks
                        chunk_preview = chunk.text[:100]
                        if len(chunk.text) > 100:
                            chunk_preview += "..."
                        print(f"   ‚Ä¢ Chunk {j+1}: {chunk_preview}")
                    if len(result.chunks) > 3:
                        print(f"   ‚Ä¢ ... and {len(result.chunks) - 3} more chunks")
                    print()
                
            else:
                print(f"‚ùå Failed: {result.error_message}")
                print()
                
        except Exception as e:
            print(f"üí• Exception: {str(e)}")
            print()
        
        # Add delay between requests to be respectful
        if i < len(test_urls):
            print("‚è≥ Waiting before next request...")
            await asyncio.sleep(2)
            print()


async def demo_batch_processing():
    """Demonstrate batch processing of multiple URLs."""
    print("üöÄ Batch Processing Demo")
    print("=" * 50)
    
    web_processor = WebProcessor()
    config = WebScrapingConfig(
        timeout=5,
        rate_limit_delay=0.5,
        convert_to_markdown=True
    )
    
    # Multiple URLs for batch processing
    urls = [
        "https://httpbin.org/html",
        "https://example.com",
        "https://httpbin.org/robots.txt",  # This will fail due to content type
    ]
    
    print(f"üìã Processing {len(urls)} URLs in batch...")
    print()
    
    try:
        # Process all URLs
        results = await web_processor.process_urls(urls, config)
        
        # Summary
        successful = [r for r in results if r.success]
        failed = [r for r in results if not r.success]
        
        print(f"üìä Batch Results:")
        print(f"   ‚Ä¢ Total URLs: {len(urls)}")
        print(f"   ‚Ä¢ Successful: {len(successful)}")
        print(f"   ‚Ä¢ Failed: {len(failed)}")
        print()
        
        # Show successful results
        if successful:
            print("‚úÖ Successful extractions:")
            for result in successful:
                print(f"   ‚Ä¢ {result.url}")
                print(f"     - Title: {result.content.title}")
                print(f"     - Content: {result.content.content_length} chars")
                print(f"     - Chunks: {len(result.chunks)}")
                print(f"     - Time: {result.processing_time:.2f}s")
            print()
        
        # Show failed results
        if failed:
            print("‚ùå Failed extractions:")
            for result in failed:
                print(f"   ‚Ä¢ {result.url}")
                print(f"     - Error: {result.error_message}")
            print()
            
    except Exception as e:
        print(f"üí• Batch processing failed: {str(e)}")
        print()


async def demo_configuration_options():
    """Demonstrate different configuration options."""
    print("‚öôÔ∏è  Configuration Options Demo")
    print("=" * 50)
    
    web_processor = WebProcessor()
    url = "https://httpbin.org/html"
    
    # Test different configurations
    configs = [
        ("Minimal extraction", WebScrapingConfig(
            extract_links=False,
            convert_to_markdown=False,
            clean_content=False
        )),
        ("Full extraction", WebScrapingConfig(
            extract_links=True,
            convert_to_markdown=True,
            clean_content=True,
            remove_navigation=True,
            remove_footer=True
        )),
        ("Fast processing", WebScrapingConfig(
            timeout=3,
            max_retries=1,
            rate_limit_delay=0.1
        ))
    ]
    
    for name, config in configs:
        print(f"üîß Testing: {name}")
        print("-" * 30)
        
        try:
            result = await web_processor.process_url(url, config)
            
            if result.success:
                content = result.content
                print(f"   ‚Ä¢ Processing time: {result.processing_time:.2f}s")
                print(f"   ‚Ä¢ Content length: {content.content_length}")
                print(f"   ‚Ä¢ Has markdown: {bool(content.markdown_content)}")
                print(f"   ‚Ä¢ Links found: {len(content.links)}")
                print(f"   ‚Ä¢ Chunks: {len(result.chunks)}")
            else:
                print(f"   ‚Ä¢ Failed: {result.error_message}")
                
        except Exception as e:
            print(f"   ‚Ä¢ Exception: {str(e)}")
        
        print()
        await asyncio.sleep(1)  # Brief delay


async def main():
    """Run all demos."""
    print("üéØ MoRAG Web Scraping Demonstration")
    print("=" * 60)
    print()
    
    try:
        # Run demos
        await demo_basic_scraping()
        await demo_batch_processing()
        await demo_configuration_options()
        
        print("üéâ Demo completed successfully!")
        print()
        print("üí° Next steps:")
        print("   ‚Ä¢ Try scraping your own URLs")
        print("   ‚Ä¢ Integrate with embedding services")
        print("   ‚Ä¢ Store results in vector database")
        print("   ‚Ä¢ Build RAG applications with the extracted content")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Demo interrupted by user")
    except Exception as e:
        print(f"\nüí• Demo failed: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # Run the demo
    asyncio.run(main())
